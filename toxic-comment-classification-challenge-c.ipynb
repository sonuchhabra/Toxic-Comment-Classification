{"cells":[{"metadata":{"_cell_guid":"572e1679-4e61-41b5-854b-339666af522f","_uuid":"7fe7b4142f3bc9f0df2eede0a695ab3710f6046f","trusted":false,"collapsed":true},"cell_type":"code","source":"import sys\nsys.getrecursionlimit()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ce9e831-18de-4d73-a17b-ee22c8260b0e","_uuid":"26baa93ecfffcd11032adf7d8dbb348b5f4a40d3","trusted":false,"collapsed":true},"cell_type":"code","source":"sys.setrecursionlimit(5000)\nsys.getrecursionlimit()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d4723779-a8a2-45fe-9a68-32507c6bdcd0","_uuid":"38da3bda18788f85bbbb29634e2485c17e858307","trusted":false},"cell_type":"code","source":"# Output all code in a chunk\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"# importing required libraries and functions\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # regular expression\nfrom nltk import word_tokenize, PorterStemmer # natural language toolkit\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc\n\n# performs exactly same as OneVsRestClassifier, using that instead\n## from skmultilearn.problem_transform import BinaryRelevance\n\n# not using as removed string.punctuations using re.sub function\n## import string\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0dc255c2-d46f-41a4-9f80-2d0409c1c2c4","_uuid":"1541435b40ca5452f846776ca0569c5c0c2203f4","trusted":false},"cell_type":"code","source":"# download nltk packages\n# nltk.download()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# reading data\ntrain = pd.read_csv(\"../input/train.csv\", nrows=40000)\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49f6b26a-9b0d-4b17-9541-07499497bf75","_uuid":"5314745d9591b2f4d78ce2f0b5de47c26268a8ac","trusted":false,"collapsed":true},"cell_type":"code","source":"# verifying data\ntrain.comment_text.head()\ntest.comment_text.head()\nlen(train)\nlen(test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"955b3d66-73ac-4fbf-8fbe-496faccecb27","_uuid":"0a3d772d117a3af068984a3732d64e0b30fdd635","trusted":false},"cell_type":"code","source":"# creating train-validation split\nX_train, X_val, y_train, y_val = train_test_split(train.comment_text, train.iloc[:,2:8], test_size=0.3, random_state=19)\nX_test = test.comment_text","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e59044f7-9978-4556-be18-aeddcf1451a7","_uuid":"61d5eb264615ce83e77a1eefab326f7654ab5953","trusted":false},"cell_type":"code","source":"# creating function to normalize text\ndef normalize(text):\n    # recognizing new line characters and tab spaces and substituting it with space\n    norm_text = re.sub(r'\\n|\\t', ' ', text)\n    # recognizing time values\n    norm_text = re.sub(r'[0-9]{1,2}:[0-9][0-9]', 'time_value', norm_text) # example 5:13pm and 05:13pm\n    # recognizing date values\n    norm_text = re.sub(r'\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}', 'date_value', norm_text) # example 2018-03/05 and 04/03-2018\n    norm_text = re.sub(r'[0-9]{1,4}[ ,][A-Za-z]{3,10}[ ,][0-9]{1,4}', 'date_value', norm_text) # example 9 june 2009 and 9 June 2009\n    # substitute characters not required by nothing, removing unrequired characters\n    norm_text = re.sub(r'[^A-Za-z_ ]', ' ', norm_text)\n    # removing multiple space values\n    norm_text = re.sub(r' +', ' ', norm_text)\n    # removing trailing spaces from front and back and converting all text to lowercase\n    norm_text = norm_text.strip().lower()\n    return norm_text","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6012029b-4d5a-4a1e-b60c-66853ee2aa25","_uuid":"859daaecd966252bb34537c8d814b922e7e28ec8","trusted":false},"cell_type":"code","source":"# creating stemmer object of PorterStemmer function\nstemmer = PorterStemmer()\n\n# writing stem_tokens function to perform stemming on tokens\ndef stem_tokens(tokens, stemmer): # tokens example: ['today', 'is', 'a', 'good', 'day']\n    stemmed = [stemmer.stem(word) for word in tokens]\n    return stemmed","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"25608e61-8979-48d9-84bf-c4bb9730b88b","_uuid":"a1db9ede702815ab493e9f7e20118536cda9c5df","trusted":false},"cell_type":"code","source":"# processing text as follows\n# tokenize words in each comment\n# remove stopwords or words upto lenght of 3 characters\n# stem words using the stem_tokens function we created above\ndef text_process(text): # text is a single sentence; for example: 'today is a good day'\n    temp_tokens = word_tokenize(text)\n\n    # using alternative to removing stopwords of english\n    ## tokens = [word for word in temp_tokens if len(word) > 3]\n    \n    # removing english stopwords, code was commented to save computation time\n    nostop_tokens = [word for word in temp_tokens if word not in stopwords.words('english')]\n    \n    stems = stem_tokens(nostop_tokens, stemmer)\n    return ' '.join(stems)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b822bf82-49e3-4200-a423-9ca1a455a188","_uuid":"b94e76843f34bddc6ea25439a9f62af130ec1b6b","trusted":false,"collapsed":true},"cell_type":"code","source":"# lenght of stopword of english\nlen(stopwords.words('english'))\nstopwords.words('english')[:10]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"667befc9-43c5-4fc3-af6b-48efcc76820c","_uuid":"5ce6ac54bc12df126585a76b90644f7aff29c3e0","trusted":false},"cell_type":"code","source":"# preparing training text to pass in count vectorizer\ncorpus = []\nfor text in X_train:\n    text = normalize(text)\n    text = text_process(text)\n    corpus.append(text)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8b7758e2-f2dd-4fb5-9068-24d9c557814b","_uuid":"bdc80c013021636cb9b65280a57562ef4828044a","trusted":false},"cell_type":"code","source":"# build Count Vectorizer, to convert a collection of text documents to a matrix of token counts\ncount_vect = CountVectorizer(ngram_range=(1,2))\nX_train_counts = count_vect.fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ddf1fda7-538e-4b5e-9857-0f27cbba096f","_uuid":"ee1025cf12d58982bb9b0c5c32d4e5b17fec3db8","trusted":false},"cell_type":"code","source":"# build TFIDF Transformer, to transform a count matrix to a normalized tf or tf-idf representation\n# tfidf - term frequency inverse document frequency\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"97a1aa6c-a205-48c8-8dba-d4b5a493df1a","_uuid":"d24e8e9a8b42009e1d1542ec5504fc405dee5777","trusted":false},"cell_type":"code","source":"# verifing data\n# print(X_train_counts.toarray())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b03ca8f2-7244-409e-b901-0ec136494931","_uuid":"0e4f4a426778b51eb69e05835f581c53e38a95de","trusted":false},"cell_type":"code","source":"# verifing data\n# print(X_train_tfidf.toarray())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6bd0cc52-aef2-459d-8b4e-f49e5ffd94cc","_uuid":"c34e5d64b6c20b08381216a470f07a145219407c","trusted":false},"cell_type":"code","source":"# checking how much text is transformed\ntemp = pd.DataFrame({'Before': X_train, 'After': corpus})\nprint(temp.sample(10))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"952932e0-afdb-4e38-abb4-9b4ff42b059a","_uuid":"5a0794c85a4c279b186742aa7d224800cb30fd2d","trusted":false},"cell_type":"code","source":"# preparing validation text to pass in count vectorizer\nX_val_set = []\nfor text in X_val:\n    text = normalize(text)\n    text = text_process(text)\n    X_val_set.append(text)\n\n# tranforming validation data using count vectorizer followed by tfidf transformer\nX_val_counts = count_vect.transform(X_val_set)\nX_val_tfidf = tfidf_transformer.transform(X_val_counts)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"42516970-fe5a-4356-afee-6cc634bfd529","_uuid":"c732481dfe521fc368db06f19893f2e49668c86f","trusted":false},"cell_type":"code","source":"# preparing test text to pass in count vectorizer\nX_test_set = []\nfor text in X_test:\n    text = normalize(text)\n    text = text_process(text)\n    X_test_set.append(text)\n\n# tranforming validation data using count vectorizer followed by tfidf transformer\nX_test_counts = count_vect.transform(X_test_set)\nX_test_tfidf = tfidf_transformer.transform(X_test_counts)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c61868f1-b9d8-4b1e-9c59-c27682cef5c8","_uuid":"4c2c60061cb0bf3bde5079ecdfaa617bad880e26","trusted":false},"cell_type":"code","source":"# creating dictionary to store prediction results\nresult_test = dict()\nresult_val = dict()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"88820494-ea53-42d5-a435-9329f24aee0b","_uuid":"ec58d0a03b4bcf6c65b104183f21707e1c881aee","trusted":false},"cell_type":"code","source":"# Multinomial Naive Bayes Model\nMNB_classifier = OneVsRestClassifier(MultinomialNB())\ngrid_values = {'estimator__alpha': [0.001, 0.01, 0.1, 1.0, 10, 100]}\nMNB_model = GridSearchCV(MNB_classifier, param_grid = grid_values, scoring = 'roc_auc')\nMNB_model.fit(X_train_tfidf, y_train)\nprint('Accurary of Multinomial Naive Bayes Classifier on Training Data: {:.3f}' .format(MNB_model.score(X_train_tfidf, y_train)))\nprint('Accurary of Multinomial Naive Bayes Classifier on Validation Data: {:.3f}' .format(MNB_model.score(X_val_tfidf, y_val)))\nprint('Grid best parameter (max. accuracy): ', MNB_model.best_params_)\nprint('Grid best score (accuracy): ', MNB_model.best_score_)\nresult_test['Multinomial_NB'] = MNB_model.predict_proba(X_test_tfidf)\nresult_val['Multinomial_NB'] = MNB_model.predict_proba(X_val_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6833ca09-0779-49cd-8ff4-fa4ecbdfd70a","_uuid":"498417b44edc00e3580a2ba9f49640a7a600fb09","trusted":false},"cell_type":"code","source":"result_test['Multinomial_NB'].sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60b63917-ce38-4d66-af52-19d62507e649","_uuid":"038bef7ba8961a593c0e7de9d85361a83a41a32d"},"cell_type":"markdown","source":"# Bernoulli Naive Bayes Model\nBNB_classifier = OneVsRestClassifier(BernoulliNB())\ngrid_values = {'estimator__alpha': [0.001, 0.01, 0.1, 1.0, 10, 100]}\nBNB_model = GridSearchCV(BNB_classifier, param_grid = grid_values, scoring = 'roc_auc')\nBNB_model.fit(X_train_tfidf, y_train)\nprint('Accurary of Bernoulli Naive Bayes Classifier on Training Data: {:.3f}' .format(BNB_model.score(X_train_tfidf, y_train)))\nprint('Accurary of Bernoulli Naive Bayes Classifier on Validation Data: {:.3f}' .format(BNB_model.score(X_val_tfidf, y_val)))\nprint('Grid best parameter (max. accuracy): ', BNB_model.best_params_)\nprint('Grid best score (accuracy): ', BNB_model.best_score_)\nresult_test['Bernoulli_NB'] = BNB_model.predict_proba(X_test_tfidf)\nresult_val['Bernoulli_NB'] = BNB_model.predict_proba(X_val_tfidf)"},{"metadata":{"_cell_guid":"4c3e9a51-770a-4e64-87db-9099c2574ebb","_uuid":"e2cc4c58e4733e266ec902976ee3e5dd5afaeeab"},"cell_type":"markdown","source":"result_test['Bernoulli_NB'].sum(axis=0)"},{"metadata":{"collapsed":true,"_cell_guid":"77e0f335-4b9c-4446-9d47-ee94c1bedbb8","_uuid":"d0ce1cc89509297584bf44c0bde3ca6f5ec41d39","trusted":false},"cell_type":"code","source":"# Logistic Regression Model, part 1\nlog_model = OneVsRestClassifier(LogisticRegression())\nlog_model.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0770729c-a7a1-4ce2-8271-2a49eb7b6f7d","_uuid":"cea955f976a9da17ace81ce2cf98ff52496198cc","trusted":false},"cell_type":"code","source":"# Logistic Regression Model, part 2\ngrid_values = {'estimator__C': [0.3, 1.0, 30.0]}\nlog_grid = GridSearchCV(log_model, param_grid = grid_values, scoring = 'roc_auc')\nlog_grid.fit(X_train_tfidf, y_train)\nprint('Accurary of Logistic Regression Classifier on Training Data: {:.3f}' .format(log_grid.score(X_train_tfidf, y_train)))\nprint('Accurary of Logistic Regression Classifier on Validation Data: {:.3f}' .format(log_grid.score(X_val_tfidf, y_val)))\nprint('Grid best parameter (max. accuracy): ', log_grid.best_params_)\nprint('Grid best score (accuracy): ', log_grid.best_score_)\nresult_test['Logistic_Regression'] = log_grid.predict_proba(X_test_tfidf)\nresult_val['Logistic_Regression'] = log_grid.predict_proba(X_val_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1bd226a1-2131-45af-8af7-6cbe51f30e02","_uuid":"c4597adb09e2a92e77219850e1a45971b3b2d820"},"cell_type":"markdown","source":"result_test['Logistic_Regression'].sum(axis=0)"},{"metadata":{"collapsed":true,"_cell_guid":"3c6eacb3-f166-45d7-ba80-0561b9e292a7","_uuid":"2cb07bfe61b70aed56832f9789d9a75e1cb26392"},"cell_type":"markdown","source":"# SVM Classifier Model\ngrid_values = {'estimator__C': [0.3, 1.0, 30.0]}\nsvm_model = OneVsRestClassifier(SVC(kernel = 'linear'))\nsvm_grid = GridSearchCV(svm_model, param_grid = grid_values, scoring = 'roc_auc')\nsvm_grid.fit(X_train_tfidf, y_train)\nprint('Accurary of SVM Classifier on Training Data: {:.3f}' .format(svm_grid.score(X_train_tfidf, y_train)))\nprint('Accurary of SVM Classifier on Validation Data: {:.3f}' .format(svm_grid.score(X_val_tfidf, y_val)))\nprint('Grid best parameter (max. accuracy): ', svm_grid.best_params_)\nprint('Grid best score (accuracy): ', svm_grid.best_score_)\nresult_test['SVM_Classifier'] = svm_grid.predict_proba(X_test_tfidf)\nresult_val['SVM_Classifier'] = svm_grid.predict_proba(X_val_tfidf)"},{"metadata":{"collapsed":true,"_cell_guid":"c4e475ec-111c-43b1-a15c-a9f134d4eace","_uuid":"422b842de9c5eb090b0fa7d483e819d83b544955"},"cell_type":"markdown","source":"result_test['SVM_Classifier'].sum(axis=0)"},{"metadata":{"collapsed":true,"_cell_guid":"583824f9-1833-4fcf-9ecf-16935d053f5a","_uuid":"6b21f31255c8ea4cfc85e6be55d7ee4be3fae0a1"},"cell_type":"markdown","source":"# how many positive cases, i.e toxic cases we recognized for each model?\nprint('Number of Toxic Cases using Multinomial Naive Bayes Model: {:.2f}' .format(result_test['Multinomial_NB'].sum()))\nprint('Number of Toxic Cases using Bernoulli Naive Bayes Model: {:.2f}' .format(result_test['Bernoulli_NB'].sum()))\nprint('Number of Toxic Cases using Logistic Regression Classifier Model: {:.2f}' .format(result_test['Logistic_Regression'].sum()))\nprint('Number of Toxic Cases using SVM Classifier Model: {:.2f}' .format(result_test['SVM_Classifier'].sum()))\n\n# predicted for how many comments?\nprint('\\nTotal Number of Comments for which we made Predictions: {:.2f}' .format(len(X_test)))\n\n# number of positive cases in training data and length of training data, includes validation data\nprint('\\nTotal Number of Positive Cases in Training Data (Training + Validation): {:.2f}' .format(train.iloc[:,2:8].sum(axis=0).sum()))\nprint('Total Number of Comments in Training Data (Training + Validation): {:.2f}' .format(len(X_train)+len(X_val)))"},{"metadata":{"collapsed":true,"_cell_guid":"9f5ea174-c6e3-4712-9119-2923740ffbcf","_uuid":"c866bfadb174fe3c35e60fb0be65424940a5f34f","trusted":false},"cell_type":"code","source":"# storing results of SVM Classifier as our result\ny_test = result_test['Logistic_Regression']\ntype(y_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"34d7721b-5e3a-4d2b-8b9c-5acdcb4be00c","_uuid":"62687a65bfcdf4de4eb8c644054219d515fb553a","trusted":false},"cell_type":"code","source":"# combining final results with the original test data set\noutput = pd.DataFrame(y_test, columns = train.columns[2:8], index = test.index)\noutput = pd.concat([test, output], axis=1)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0878b2b8-fe9c-44b5-943f-a6669467cf98","_uuid":"6abdc8704a7c65258df8f9452012b6e2544efeab","trusted":false},"cell_type":"code","source":"# verifing data\noutput.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e0228c73-7fe0-4e63-ae18-6630b2cbb190","_uuid":"8ad24f43eecd64a4eda0622a698f1296e6175236","trusted":false},"cell_type":"code","source":"# verifing select random case, as per index from above code chunk\noutput.iloc[5902,:]\noutput.comment_text[5902]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5d047093-d92e-4906-bde0-29e0edf99edb","_uuid":"bc9cfafb065d8aa78b69e891af10562cc4eb6032","trusted":false},"cell_type":"code","source":"# quick summary for training, validation and test set respectively\ny_train.sum(axis=0)\ny_val.sum(axis=0)\noutput.iloc[:,2:8].sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7b1513cf-ec6f-45e6-a996-948675583326","_uuid":"d99c68b83a2b9ba3939b898a19f9cf6622b96a00","trusted":false},"cell_type":"code","source":"#ngrams, is it unigram or bigram or mix?\n#alpha parameter for Naive Bayes\n#truncatesvd\n#precision recall\n#visualizations","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"817f1443-143d-4395-9c22-a339f8696c1b","_uuid":"e4197832c72b239794e45a821c80d171922a4192"},"cell_type":"markdown","source":"from sklearn import metrics\nprint(metrics.classification_report(y_val.toxic, result_val['Multinomial_NB'][:,1], target_names = [\"positive\", \"negative\"]))\nprint(metrics.classification_report(y_val.toxic, result_val['Bernoulli_NB'][:,1], target_names = [\"positive\", \"negative\"]))\nprint(metrics.classification_report(y_val.toxic, result_val['Logistic_Regression'][:,1], target_names = [\"positive\", \"negative\"]))\nprint(metrics.classification_report(y_val.toxic, result_val['SVM_Classifier'][:,1], target_names = [\"positive\", \"negative\"]))"},{"metadata":{"collapsed":true,"_cell_guid":"72d20c70-98f2-4bab-b433-537005b83265","_uuid":"f9b192ccbaceee844d378b0ac669c9a21942afaa","trusted":false},"cell_type":"code","source":"my_submission = output.drop(['comment_text'], axis = 1, inplace = False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5ee3e0ac-f1ad-40c1-a0fa-43d69d5488c4","scrolled":true,"_uuid":"724441a6756ed9f8d0911baea7d26720e4325da6","trusted":false},"cell_type":"code","source":"type(output)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b4238e3a-4175-4690-a676-5f8a4da65d8a","_uuid":"8d18486fcb720eafe382cd60bb34dcabd88c9ae7","trusted":false},"cell_type":"code","source":"my_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"bc94f7e5-f9aa-41c3-a065-28837642c7ae","_uuid":"89342de4742f53f6a284118ab94f7775944eb17d","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"dee0dc38-959c-4be1-a27c-2017591ce508","_uuid":"3859958f12219ba3294a90d8fcc8eff009885c36","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}